import pickle       # To store processed data to save time
import os           # To access directories
from csv import reader  # To read label data
from sklearn import svm  # To create the SVM model
import dlib     # For facial recognition
import cv2      # To read images
import numpy as np  # For data processing
from matplotlib import pyplot as plt    # To generate figures
from sklearn.metrics import plot_confusion_matrix   # To generate confusion matrices
from sklearn import model_selection     # To use the train test split function

# Main function called from the main file
def main(sel, model, cfm):
    if sel==0:      # User input arg 1 - Selects which mode to use: Arg1==0 creates the model to use
        print("Task A2: Smile Detection")
        model = create_model()  # Create model function called to create model object with desired parameters

        return model    # Returns model object to main file
    elif sel==1:    # User input arg 1 - Arg1==1 trains the model and validates it
        directory = 'celeba/'   # Directory of the dataset for training
        y_name, y_gen, y_smile, face_list, points = import_data(directory)  # Imports data from csv file as well as pickle file if found. If not, then the data is generated by this function.
        print("No. of faces detected in this set: ", len(face_list))    # face_list contains an index of all detected faces
        x_use, y_use = process(points, face_list, y_smile)  # Process function flattens the data
        x_train, x_vald, y_train, y_vald = model_selection.train_test_split(x_use, y_use, train_size=0.8, random_state=0)   # x,y data is split with a gives percentage and seed
        print("Number of training samples: ", len(x_train))
        model = train_model(model, x_train, y_train)    # Model is trained using x and y training data
        print("Number of validation samples: ", len(x_vald))
        accuracy = test_model(model, x_vald, y_vald)    # Model is validated using x and y validation data
        print("Accuracy of validation set: ", str(accuracy), "\n")
        if cfm==1:  # Use input arg 3 - Arg1==1 plots the confusion matrix
            disp = plot_confusion_matrix(model, x_vald, y_vald, cmap=plt.cm.Blues)  # Generate confusion matrix
            print(disp.confusion_matrix)
            plt.show()

        return accuracy     # Returns the accuracy of the model using validation data to main file
    elif sel==2:    # User input arg 1 - Arg1==2 tests the model using a test set of data
        directory = 'celeba_test/'  # Directory of dataset for testing
        y_name, y_gen, y_smile, face_list, points = import_data(directory)  # Imports data from csv file as well as pickle file if found. If not, then the data is generated by this function.
        print("No. of faces detected in this set: ", len(face_list))    # face_list contains an index of all detected faces
        x_use, y_use = process(points, face_list, y_smile)  # Process function flattens the data
        print("Number of test samples: ", len(x_use))
        accuracy = test_model(model, x_use, y_use)  # Model is tested using x and y test data
        print("Accuracy of unseen test set: ", str(accuracy), "\n")
        if cfm==1:  # Use input arg 3 - Arg1==1 plots the confusion matrix
            disp = plot_confusion_matrix(model, x_use, y_use, cmap=plt.cm.Blues)    # Generate confusion matrix
            print(disp.confusion_matrix)
            plt.show()

        return accuracy     # Returns the accuracy of the model using test data to main file

# Creates SVM
def create_model():
    print("Creating model...")
    clf = svm.SVC(kernel='rbf', probability=True, C=10, gamma=1e-4)     # Create model object using selected hyperparameters

    return clf  # The model object is passed back to the main function

# Train SVM
def train_model(model, x_train, y_train):
    print("Training model...")
    model = model.fit(x_train, y_train)     # Model is trained using x and y train data
    print("Model training finished")

    return model    # The trained model is passed back to the main function

# Testing mdoel
def test_model(model, x_t, y_t):
    print("Testing model...")
    accuracy = model.score(x_t, y_t)    # Model is trained using x and y test data
    print("Model testing finished")

    return accuracy     # The model accuracy is passed back to the main function

# Flatten raw coordinates
def process(data, face_list, y_use):
    use_pic = len(face_list)    # Number of usable images
    dat_pr = []     # Create an empty list to append data to
    y_proc = []     # Create an empty list to append data to
    for i in range(0, use_pic):     # For each usable picture
        idx = face_list[i]  # References the face_list to generate index to access correct entries in the label data
        temp = []   # Empty list to append coordinates to
        for m in range(0, 68):  # For each landmark
            for n in range(0, 2):   # For x and y
                temp.append(data[idx][m][n])    # The temporary list append each x and y coordinate for each landmark
        dat_pr.append(temp)     # The empty data list appends each temporary list containing flattened landmark coordinates for each image
        y_proc.append(y_use[idx])   # Empty list appends correct label from position generated by face_list

    return dat_pr, y_proc   # Returns the flattened coordinates and y labels for each usable image

# Imports csv data. If pickle file is available, imports data, else generates it.
def import_data(directory):
    print("Acquiring labels and landmark data...")
    # Directories for each file is generated as each image is in a sub folder of celeba
    full_dir = str(os.path.dirname(__file__)[:-2])+'/Datasets/'+directory
    csv_src = os.path.join(full_dir, "labels.csv")
    img_src = os.path.join(full_dir, "img/")

    y_name = []     # Stores the file name for each image
    y_gen = []  # Stores gender data for each image
    y_smile = []    # Stores smile data for each image
    face_list = []  # Stores a list of all images where a face is detected
    with open(csv_src) as file:     # Opens the labels.csv file
        dat_read = reader(file, delimiter='\t')  # Data is tab spaced
        temp = list(dat_read)[1:]  # Remove first element of list (headers)
        for n in range(0, len(temp)):  # Go through each row
            y_name.append(temp[n][1])  # Second element is the name
            y_gen.append(temp[n][2])  # Third element is gender
            y_smile.append(temp[n][3])  # Fourth element is smiling or not
    no_pics = len(y_name)   # Total number of images (NOT total number of faces)

    if not os.path.isfile(full_dir+'facial_landmarks.pickle'):  # If pickle data file containing facial landmarks doesnt exist
        print("Landmark data not found - generating...")
        detector = dlib.get_frontal_face_detector()  # Generate detector object using dlib's face detector
        predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')  # Generate landmark predictor using dlib's 68 landmarks model
        points = np.array([[[None] * 2] * 68] * no_pics)  # 68 landmark coordinates stored here as generated

        for i in range(0, no_pics): # Open image and find face
            src = img_src + y_name[i]  # Generating path for each image
            gray = cv2.imread(src, 0)  # Image is read (using path) in grayscale - easier to process
            faces = detector(gray)  # Run the frontal face detector on the grayscale image

            if list(faces):  # If a face is detected i.e. faces object contains data
                face_list.append(i)  # Keep track of iteration number in a list of detected faces
                face = list(faces)[0]  # Takes data from first element of faces (the detector can detect multiple faces)
                landmarks = predictor(gray, face)  # Run the dlib 68 landmarks predictor on gray image in face region
                for n in range(0, 68):  # For each of the 68 landmarks
                    points[i][n][0] = landmarks.part(n).x  # Save the x coordinate of the nth landmark
                    points[i][n][1] = landmarks.part(n).y  # Save the y coordinate of the nth landmark

        points = list(points)   # Converts the points into a list - this means pickle data can be read by a script without numpy
        face_list = list(face_list)     # Convert the face_list into a list
        points.append(face_list)    # Combines the points and face_list array into a single variable to store in a single pickle file
        with open(full_dir+'facial_landmarks.pickle', 'wb') as f1:  # A pickle file is opened (created if it doesnt exist)
            pickle.dump(points, f1)  # Dump points data to pickle file

    with open(full_dir+'facial_landmarks.pickle', 'rb') as f2:  # Open the pickle file containing the landmark data
        temp1 = pickle.load(f2)  # Load the file into a temporary variable
        points = temp1  # Turn the temporary file object into a list
        face_list = points[-1]  # Extract the face_list from combined matrix (as it was appended)
        points = points[0:-1]  # Extract landmark data into one list (as it was joined with face_list)

    return y_name, y_gen, y_smile, face_list, points    # The labels, face_list and landmark data is passed back to the main function



